{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from judge import LLMJudge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from google.generativeai.types.safety_types import HarmBlockThreshold, HarmCategory\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from deepeval.metrics import (AnswerRelevancyMetric,\n",
    "                              BiasMetric,\n",
    "                            )\n",
    "from deepeval.test_case import LLMTestCase\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from google.generativeai.types.safety_types import HarmBlockThreshold, HarmCategory\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "import asyncio\n",
    "class OpenAiDeepEval(ChatOpenAI):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def get_model_name(self):\n",
    "        return self.model_name\n",
    "\n",
    "class GoogleGemenai(DeepEvalBaseLLM):\n",
    "    \"\"\"Class to implement Vertex AI for DeepEval\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def make_model():\n",
    "        return ChatGoogleGenerativeAI(\n",
    "                model=\"gemini-1.5-flash\",\n",
    "                temperature=0,\n",
    "                max_tokens=None,\n",
    "                timeout=None,\n",
    "                max_retries=2,\n",
    "                safety_settings={\n",
    "                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                },\n",
    "        )\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"gemenai\"\n",
    "    \n",
    "class LLMJudge:\n",
    "    def __init__(self):\n",
    "        self.llm = OpenAiDeepEval(model=os.environ.get('OPENAI_LLM_JUDGE_MODEL','gpt-4o-mini'),\n",
    "                              temperature=0,\n",
    "                              api_key=os.environ.get('OPENAI_API_KEY'))  # Default LLM, can be changed later\n",
    "        self.prompts_path = {}\n",
    "\n",
    "    def set_llm(self, new_llm):\n",
    "        self.llm = new_llm\n",
    "\n",
    "    async def answer_relevancy(self, input:str, llm_response:str):\n",
    "        \"\"\"\n",
    "        This fucntion will take input and llm_response and return the relevancy score \\n\n",
    "        calculated by the LIBRARY Deepeval\n",
    "        #TODO need to check the async_mode to true and get response async\n",
    "        \"\"\"\n",
    "        metric =  AnswerRelevancyMetric(threshold=0.7,\n",
    "                                            model=self.llm.get_model_name(),\n",
    "                                            include_reason=True,\n",
    "                                            async_mode=True,\n",
    "                                        )\n",
    "        \n",
    "\n",
    "        test_case = LLMTestCase(input=input, actual_output=llm_response)\n",
    "        metric.measure(test_case)\n",
    "\n",
    "        return str({\"reason\":metric.reason,\n",
    "                    \"numeric\":metric.score})\n",
    "    \n",
    "    async def answer_bias(self, input:str, llm_response:str):\n",
    "        \"\"\"\n",
    "        This fucntion will take input and llm_response and return the bias score \\n\n",
    "        calculated by the LIBRARY Deepeval\n",
    "        #TODO need to check the async_mode to true and get response async\n",
    "        \"\"\"\n",
    "        metric =  BiasMetric(threshold=0.7,\n",
    "                                            model=self.llm.get_model_name(),\n",
    "                                            include_reason=True,\n",
    "                                            async_mode=True,\n",
    "                                        )\n",
    "        \n",
    "\n",
    "        test_case = LLMTestCase(input=input, actual_output=llm_response)\n",
    "        metric.measure(test_case)\n",
    "        return str({\"reason\":metric.reason,\n",
    "                    \"numeric\":metric.score})\n",
    "    \n",
    "async def call_metrics():\n",
    "    j = LLMJudge()\n",
    "    question = \"hey what is my name\"\n",
    "    response = 'my name'\n",
    "    async with asyncio.TaskGroup() as tg:\n",
    "        results = [tg.create_task(j.answer_relevancy(question,response)),\n",
    "                   tg.create_task(j.answer_bias(question,response))]\n",
    "    \n",
    "    # results = await asyncio.gather(*a)\n",
    "    return results\n",
    "    \n",
    "async def main():\n",
    "    results = await call_metrics()\n",
    "    print(results)\n",
    "\n",
    "asyncio.run(main())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = LLMJudge()\n",
    "question = \"hey what is my name\"\n",
    "response = 'my name'\n",
    "a = [asyncio.create_task(j.answer_relevancy(question,response)),asyncio.create_task(j.answer_bias(question,response))]\n",
    "\n",
    "results = asyncio.gather(*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results._state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = ChatOpenAI(model='gpt-4o-mini',\n",
    "                                  temperature=0,\n",
    "                                  api_key=os.environ.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
